{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization machines mathematically represented :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $w_0$ is the bias term, $w_i$ is the linear term for the $i$-th feature, $v_i$ is the factor vector for the $i$-th feature, and $\\langle \\cdot, \\cdot \\rangle$ is the inner product.\n",
    "\n",
    "The interaction term can be shown to be equivalent to rowwise sum of XV element wise squared minus rowwise sum of (X squared element wise squared * V squared element wise squared) divided by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful references:\n",
    "https://www.kaggle.com/code/gennadylaptev/factorization-machine-implemented-in-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#factorization machine in pytorch\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, n, k):\n",
    "        \"\"\"   \n",
    "        n: number of features\n",
    "        k: number of latent factors\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.linear = nn.Linear(n, 1, bias=True)\n",
    "        self.v = nn.Parameter(torch.randn(n, k))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a batch of samples, each sample has n features\n",
    "        # linear part\n",
    "        linear = self.linear(x)\n",
    "        # factorization machine part\n",
    "        # first order interaction\n",
    "        x = x.unsqueeze(2) # unsqueeze to make it 3-dim\n",
    "        v = self.v.unsqueeze(0)\n",
    "        inter_1 = torch.bmm(x, v) # batch matrix multiplication\n",
    "        inter_1 = torch.pow(inter_1, 2) # element-wise power to obtain the square\n",
    "        \n",
    "        # second order interaction\n",
    "        inter_2 = torch.pow(x, 2) # element-wise power to obtain the square\n",
    "        inter_2 = torch.bmm(inter_2, torch.pow(v, 2)) # batch matrix multiplication\n",
    "        \n",
    "        fm = 0.5 * torch.sum(inter_1 - inter_2, dim=1)\n",
    "        fm = fm.squeeze(1) # squeeze to make it 2-dim\n",
    "        # output\n",
    "        out = linear + fm\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('torch_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aed997d0aaf95c760102dbf482baf2f118022ed75dc9f8ae7e129356735819f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
