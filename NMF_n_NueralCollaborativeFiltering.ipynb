{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a recommender system on the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "186\t302\t3\t891717742\n",
      "22\t377\t1\t878887116\n",
      "244\t51\t2\t880606923\n",
      "166\t346\t1\t886397596\n",
      "298\t474\t4\t884182806\n",
      "115\t265\t2\t881171488\n",
      "253\t465\t5\t891628467\n",
      "305\t451\t3\t886324817\n",
      "6\t86\t3\t883603013\n"
     ]
    }
   ],
   "source": [
    "! head data/ml-100k/u.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the ml100k dataset\n",
    "df = pd.read_csv('data/ml-100k/u.data', sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique users and items\n",
    "print(df.user_id.unique().shape[0]), print(df.item_id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# number of ratings\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead, lets create a function which creates a dictionary of user ids and item ids\n",
    "def create_user_item_dict(df):\n",
    "    user_ids = df.user_id.unique().tolist()\n",
    "    item_ids = df.item_id.unique().tolist()\n",
    "    user2idx = {o:i for i,o in enumerate(user_ids)}\n",
    "    item2idx = {o:i for i,o in enumerate(item_ids)}\n",
    "    return user2idx, item2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user and item dictionaries\n",
    "user2idx, item2idx = create_user_item_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75220</th>\n",
       "      <td>807</td>\n",
       "      <td>1411</td>\n",
       "      <td>1</td>\n",
       "      <td>893082619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48955</th>\n",
       "      <td>474</td>\n",
       "      <td>659</td>\n",
       "      <td>5</td>\n",
       "      <td>887925187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44966</th>\n",
       "      <td>463</td>\n",
       "      <td>268</td>\n",
       "      <td>4</td>\n",
       "      <td>877384940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13568</th>\n",
       "      <td>139</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>879537844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92727</th>\n",
       "      <td>621</td>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "      <td>883799651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating  timestamp\n",
       "75220      807     1411       1  893082619\n",
       "48955      474      659       5  887925187\n",
       "44966      463      268       4  877384940\n",
       "13568      139      286       4  879537844\n",
       "92727      621      751       4  883799651"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class that does the encoding too\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df, user2idx, item2idx):\n",
    "        self.users = df.user_id.values\n",
    "        self.items = df.item_id.values\n",
    "        self.ratings = df.rating.values\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user2idx.get(self.users[idx], -1)\n",
    "        item = self.item2idx.get(self.items[idx], -1)\n",
    "        rating = self.ratings[idx]\n",
    "        return user, item, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset\n",
    "train_ds = MovieLensDataset(train, user2idx, item2idx)\n",
    "test_ds = MovieLensDataset(test, user2idx, item2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dataset class\n",
    "train_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dataloader\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([316, 257,  99,  ..., 207, 182, 203]),\n",
       " tensor([100, 694, 201,  ...,  12, 616, 404]),\n",
       " tensor([3, 1, 4,  ..., 2, 2, 5])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([360, 144, 150,  ..., 157, 162, 221]),\n",
       " tensor([ 347, 1015,  328,  ..., 1266, 1297,   77]),\n",
       " tensor([4, 3, 4,  ..., 2, 5, 3])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Trying Matrix Factorization model\n",
    "- Here Embedding layer is used to learn the latent factors\n",
    "- GMF model without biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model class\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=40):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        #initialize the embeddings, this is important. Else, model can struggle to learn\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
    "\n",
    "\n",
    "# create a model class with bias\n",
    "class MatrixFactorizationBias(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=40):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        #initialize the embeddings, this is important. Else, model can struggle to learn\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_bias.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_bias.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1) + self.user_bias(user).squeeze() + self.item_bias(item).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the model\n",
    "model = MatrixFactorizationBias(len(user2idx), len(item2idx), n_factors=60).to(device)\n",
    "\n",
    "# define the loss function. We will use mean squared error because we are predicting ratings\n",
    "# if we were predicting a binary outcome, we would use binary cross entropy\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "# define the number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of batches\n",
    "len(train_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 14.0585: 100%|██████████| 79/79 [00:00<00:00, 206.51it/s]\n",
      "Epoch 1, Loss: 12.7910: 100%|██████████| 79/79 [00:00<00:00, 194.36it/s]\n",
      "Epoch 2, Loss: 12.7606: 100%|██████████| 79/79 [00:00<00:00, 217.65it/s]\n",
      "Epoch 3, Loss: 12.3909: 100%|██████████| 79/79 [00:00<00:00, 214.03it/s]\n",
      "Epoch 4, Loss: 14.2272: 100%|██████████| 79/79 [00:00<00:00, 187.84it/s]\n",
      "Epoch 5, Loss: 12.1089: 100%|██████████| 79/79 [00:00<00:00, 221.73it/s]\n",
      "Epoch 6, Loss: 13.3881: 100%|██████████| 79/79 [00:00<00:00, 207.65it/s]\n",
      "Epoch 7, Loss: 11.7460: 100%|██████████| 79/79 [00:00<00:00, 234.60it/s]\n",
      "Epoch 8, Loss: 11.6867: 100%|██████████| 79/79 [00:00<00:00, 189.13it/s]\n",
      "Epoch 9, Loss: 11.8292: 100%|██████████| 79/79 [00:00<00:00, 178.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the model.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    train_tqdm = tqdm(train_dl)\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_tqdm):\n",
    "        users, items, ratings = data\n",
    "        users = users.long().to(device)\n",
    "        items = items.long().to(device)\n",
    "        ratings = ratings.float().to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(users, items)\n",
    "        loss = criterion(outputs, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_tqdm.set_description(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the ratings for the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i, data in enumerate(test_dl):\n",
    "    model.eval()\n",
    "    users, items, ratings = data\n",
    "    users = users.long().to(device)\n",
    "    items = items.long().to(device)\n",
    "    ratings = ratings.float().to(device)\n",
    "    \n",
    "    outputs = model(users, items)\n",
    "    y_true.extend(ratings.cpu().detach().numpy())\n",
    "    y_pred.extend(outputs.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.287557"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Trying Neural collaborative filtering model\n",
    "- Variant 1 with MLP only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### neural collaborative filtering model\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=40):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        #initialize the embeddings, this is important. Else, model can struggle to learn\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.01)\n",
    "        \n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        nn.init.constant_(self.user_bias.weight, 0)\n",
    "        nn.init.constant_(self.item_bias.weight, 0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2*n_factors, 10) # 2*n_factors because we are concatenating user and item embeddings\n",
    "        self.fc2 = nn.Linear(10, 1) # 1 because we are predicting a single value\n",
    "        \n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_vec = self.user_factors(user)\n",
    "        item_vec = self.item_factors(item)\n",
    "        user_bias = self.user_bias(user)\n",
    "        item_bias = self.item_bias(item)\n",
    "        \n",
    "        x = torch.cat([user_vec, item_vec], dim=1) # concatenate user and item embeddings. Dimensions are batch_size x (2*n_factors)\n",
    "        x = F.relu(self.fc1(x)) # apply a non-linear activation function. Dimensions are batch_size x 20\n",
    "        x = self.fc2(x) # Dimensions are batch_size x 1\n",
    "        x = x + user_bias + item_bias # add the bias terms, Dimensions are batch_size x 1\n",
    "        return x.squeeze(1) # squeeze the output to get a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1653)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user2idx), len(item2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the model\n",
    "model = NCF(len(user2idx), len(item2idx), n_factors=5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of batches\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15697"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of model parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.2550: 100%|██████████| 79/79 [00:00<00:00, 187.59it/s]\n",
      "Epoch 1, Loss: 1.5168: 100%|██████████| 79/79 [00:00<00:00, 211.83it/s]\n",
      "Epoch 2, Loss: 1.0841: 100%|██████████| 79/79 [00:00<00:00, 218.82it/s]\n",
      "Epoch 3, Loss: 1.1753: 100%|██████████| 79/79 [00:00<00:00, 218.06it/s]\n",
      "Epoch 4, Loss: 1.2825: 100%|██████████| 79/79 [00:00<00:00, 221.89it/s]\n",
      "Epoch 5, Loss: 1.4951: 100%|██████████| 79/79 [00:00<00:00, 191.82it/s]\n",
      "Epoch 6, Loss: 1.2863: 100%|██████████| 79/79 [00:00<00:00, 214.85it/s]\n",
      "Epoch 7, Loss: 1.0250: 100%|██████████| 79/79 [00:00<00:00, 217.81it/s]\n",
      "Epoch 8, Loss: 1.3183: 100%|██████████| 79/79 [00:00<00:00, 220.65it/s]\n",
      "Epoch 9, Loss: 1.3804: 100%|██████████| 79/79 [00:00<00:00, 210.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_tqdm = tqdm(train_dl)\n",
    "    for i, data in enumerate(train_tqdm):\n",
    "        users, items, ratings = data\n",
    "        #print(users.shape, items.shape, ratings.shape)\n",
    "        users = users.long()\n",
    "        items = items.long()\n",
    "        ratings = ratings.float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(users, items)\n",
    "        loss = criterion(outputs, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_tqdm.set_description(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the ratings for the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for i, data in enumerate(test_dl):\n",
    "    model.eval()\n",
    "    users, items, ratings = data\n",
    "    users = users.long().to(device)\n",
    "    items = items.long().to(device)\n",
    "    ratings = ratings.float().to(device)\n",
    "    \n",
    "    outputs = model(users, items)\n",
    "    y_true.extend(ratings.cpu().detach().numpy())\n",
    "    y_pred.extend(outputs.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.231655"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Nueral collaborative filtering model\n",
    "- Variant 2 with MLP and GMF\n",
    "\n",
    "\n",
    "Here , NCF adopts two pathways to model users and items: 1) element-wise product of vectors, 2) concatenation of vectors. To learn interactions after concatenating of users and items latent features, the standard MLP model is applied. We allow GMF and MLP to learn separate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MLP and GMF in NCF\n",
    "# create a model class\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=40):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        #initialize the embeddings, this is important. Else, model can struggle to learn\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=40):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
    "        #initialize the embeddings, this is important. Else, model can struggle to learn\n",
    "        nn.init.normal_(self.user_factors.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_factors.weight, std=0.01)\n",
    "        \n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        nn.init.constant_(self.user_bias.weight, 0)\n",
    "        nn.init.constant_(self.item_bias.weight, 0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2*n_factors, 10) # 2*n_factors because we are concatenating user and item embeddings\n",
    "        self.fc2 = nn.Linear(10, 1) # 1 because we are predicting a single value\n",
    "        \n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_vec = self.user_factors(user)\n",
    "        item_vec = self.item_factors(item)\n",
    "        user_bias = self.user_bias(user)\n",
    "        item_bias = self.item_bias(item)\n",
    "        \n",
    "        x = torch.cat([user_vec, item_vec], dim=1) # concatenate user and item embeddings. Dimensions are batch_size x (2*n_factors)\n",
    "        x = F.relu(self.fc1(x)) # apply a non-linear activation function. Dimensions are batch_size x 20\n",
    "        x = self.fc2(x) # Dimensions are batch_size x 1\n",
    "        x = x + user_bias + item_bias # add the bias terms, Dimensions are batch_size x 1\n",
    "        return x.squeeze(1) # squeeze the output to get a single value\n",
    "\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=40):\n",
    "        super().__init__()\n",
    "        self.gmf = GMF(n_users, n_items, n_factors)\n",
    "        self.mlp = MLP(n_users, n_items, n_factors)\n",
    "        self.fc = nn.Linear(2*n_factors, 1) # 2*n_factors because we are concatenating the output of GMF and MLP\n",
    "        nn.init.kaiming_uniform_(self.fc.weight) # initialize the weights of the final layer\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        gmf = self.gmf(user, item)\n",
    "        mlp = self.mlp(user, item)\n",
    "        x = torch.cat([gmf, mlp], dim=1) # concatenate the output of GMF and MLP. Dimensions are batch_size x (2*n_factors)\n",
    "        x = self.fc(x) # Dimensions are batch_size x 1\n",
    "        return x.squeeze(1) # squeeze the output to get a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(user2idx)\n",
    "n_items = len(item2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF(n_users, n_items, n_factors=40)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of batches\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107257"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.5174: 100%|██████████| 79/79 [00:00<00:00, 164.62it/s]\n",
      "Epoch 1, Loss: 1.3356: 100%|██████████| 79/79 [00:00<00:00, 185.45it/s]\n",
      "Epoch 2, Loss: 1.5388: 100%|██████████| 79/79 [00:00<00:00, 190.42it/s]\n",
      "Epoch 3, Loss: 1.6575: 100%|██████████| 79/79 [00:00<00:00, 188.15it/s]\n",
      "Epoch 4, Loss: 1.1721: 100%|██████████| 79/79 [00:00<00:00, 186.02it/s]\n",
      "Epoch 5, Loss: 1.3692: 100%|██████████| 79/79 [00:00<00:00, 189.61it/s]\n",
      "Epoch 6, Loss: 1.5626: 100%|██████████| 79/79 [00:00<00:00, 168.10it/s]\n",
      "Epoch 7, Loss: 1.2125: 100%|██████████| 79/79 [00:00<00:00, 175.39it/s]\n",
      "Epoch 8, Loss: 1.1858: 100%|██████████| 79/79 [00:00<00:00, 185.53it/s]\n",
      "Epoch 9, Loss: 1.1830: 100%|██████████| 79/79 [00:00<00:00, 181.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_tqdm = tqdm(train_dl)\n",
    "    for i, data in enumerate(train_tqdm):\n",
    "        users, items, ratings = data\n",
    "        #print(users.shape, items.shape, ratings.shape)\n",
    "        users = users.long()\n",
    "        items = items.long()\n",
    "        ratings = ratings.float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(users, items)\n",
    "        loss = criterion(outputs, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_tqdm.set_description(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch_2_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fb8b680fd183c223b836b79cdcb0f2f0334fe2549f7dc1091ef51ac1f51f680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
